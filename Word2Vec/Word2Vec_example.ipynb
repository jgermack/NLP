{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec model  \n",
    "A corpus of Presidental inauguration texts  \n",
    "was used to create word embeddings in this example.  \n",
    "\n",
    "The texts are from the NLTK package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "#from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_dir = 'H:\\\\My Documents\\\\Work\\\\datasets\\\\pres_txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcorpus = PlaintextCorpusReader(corp_dir, '.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1965-Johnson.txt',\n",
       " '2009-Obama.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newcorpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_ID = newcorpus.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preparation  \n",
    "The function below tokenizes the words from  \n",
    "the corpus to prepare for model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not removing stop words\n",
    "result = []\n",
    "def prepText(x):\n",
    "    text = newcorpus.raw(txt_ID[x]).lower()\n",
    "    word_split = RegexpTokenizer(\"[\\w']+\")\n",
    "    words = word_split.tokenize(text)\n",
    "    result.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in range(len(txt_ID)):\n",
    "    prepText(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec model  \n",
    "A model is built from the tokenized words  \n",
    "using the default vector size of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default vector size is 100\n",
    "model = Word2Vec(result, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=2539, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_mod = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector example  \n",
    "Each word in the vocabulary is reprsented  \n",
    "by a dense vector of 100 values. The example  \n",
    "shows the vector for the word \"freedom\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0197669  -0.00939509  0.00227569  0.0460837   0.03628891 -0.00593469\n",
      "  0.03417724 -0.01471165 -0.01941761 -0.03881237  0.01652641 -0.01653039\n",
      " -0.00428944  0.00831139 -0.05865994 -0.0041921  -0.0091879  -0.02365496\n",
      "  0.09699909 -0.00142109 -0.03706247  0.00138096 -0.03337765 -0.01229199\n",
      "  0.00315145 -0.01368485 -0.00968039 -0.00724177 -0.02289274  0.03410538\n",
      " -0.02379583  0.01816357 -0.03772733  0.02510256 -0.00219953 -0.05040459\n",
      "  0.06430963  0.03038889 -0.01540066  0.02929699 -0.05042601 -0.05188082\n",
      "  0.04694723 -0.01123072 -0.0467459   0.02162767  0.0141912  -0.03862121\n",
      " -0.02790191 -0.01164071  0.02316928 -0.01294372 -0.0112659   0.0268771\n",
      " -0.00503408 -0.0014419  -0.01295909 -0.03206899 -0.04769815 -0.00863407\n",
      " -0.06970405 -0.03202133 -0.02669954 -0.02124231  0.01729305 -0.05322404\n",
      "  0.00533329  0.03407063 -0.0376511  -0.02357354 -0.02180569 -0.01571869\n",
      " -0.01386097 -0.02185489 -0.05500271  0.05997649  0.0467303   0.00758936\n",
      "  0.04518222  0.04606647 -0.04572697 -0.03184961  0.01643085 -0.04206982\n",
      "  0.01311826  0.01857078 -0.00538468 -0.01781694  0.01192844  0.00400551\n",
      "  0.03103893 -0.05984028 -0.02320671 -0.01521955 -0.01681677  0.02045002\n",
      "  0.01156237 -0.05617209  0.00701142 -0.00793566]\n"
     ]
    }
   ],
   "source": [
    "# each word has a dense vector of size 100\n",
    "print(model['freedom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_36]",
   "language": "python",
   "name": "conda-env-python_36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
